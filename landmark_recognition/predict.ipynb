{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras import backend as K\n",
    "from models.net_models import resnet50_model, vgg19_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000088da12d664db</td>\n",
       "      <td>https://lh3.googleusercontent.com/-k45wfamuhT8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001623c6d808702</td>\n",
       "      <td>https://lh3.googleusercontent.com/-OQ0ywv8KVIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001bbb682d45002</td>\n",
       "      <td>https://lh3.googleusercontent.com/-kloLenz1xZk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002362830cfe3a3</td>\n",
       "      <td>https://lh3.googleusercontent.com/-N6z79jNZYTg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000270c9100de789</td>\n",
       "      <td>https://lh3.googleusercontent.com/-keriHaVOq1U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url\n",
       "0  000088da12d664db  https://lh3.googleusercontent.com/-k45wfamuhT8...\n",
       "1  0001623c6d808702  https://lh3.googleusercontent.com/-OQ0ywv8KVIA...\n",
       "2  0001bbb682d45002  https://lh3.googleusercontent.com/-kloLenz1xZk...\n",
       "3  0002362830cfe3a3  https://lh3.googleusercontent.com/-N6z79jNZYTg...\n",
       "4  000270c9100de789  https://lh3.googleusercontent.com/-keriHaVOq1U..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance = False\n",
    "input_size = 197\n",
    "batch_size = 64\n",
    "\n",
    "data_dir = '/disk/landmark_rec/'\n",
    "test_df = pd.read_csv(data_dir+'test.csv')\n",
    "\n",
    "if balance:\n",
    "    landmark_id_map = json.load(open(data_dir+'landmark_id_map.json'))\n",
    "    landmark_id_map = {int(k): v for k, v in landmark_id_map.items()}\n",
    "    num_class = len(landmark_id_map)\n",
    "else:\n",
    "    num_class = 14951\n",
    "    \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove download failed datas\n",
    "all_ids = [re.sub('.jpg', '', os.path.basename(file_id)) for file_id in glob.glob(data_dir+'test/*.jpg')]\n",
    "test_X = test_df.loc[test_df['id'].isin(all_ids)]\n",
    "test_X = test_X['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 197, 197, 3)       12        \n",
      "_________________________________________________________________\n",
      "resnet50 (Model)             multiple                  23587712  \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "hadamard_classifier_1 (Hadam (None, 14951)             14952     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 14951)             0         \n",
      "=================================================================\n",
      "Total params: 23,602,676\n",
      "Trainable params: 23,549,550\n",
      "Non-trainable params: 53,126\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hadamard_resnet50'\n",
    "\n",
    "K.clear_session()\n",
    "model = resnet50_model(input_shape=(input_size, input_size, 3), num_classes=num_class, \n",
    "                       weight_path=os.path.join(data_dir, 'weights/{0}.hdf5'.format(model_name)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1805/1805 [07:24<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "for start in tqdm(range(0, len(test_X), batch_size)):\n",
    "    x_batch = []\n",
    "    end = min(start + batch_size, len(test_X))\n",
    "    test_X_batch = test_X[start:end]\n",
    "    for id_ in test_X_batch.values:\n",
    "        img = cv2.imread(data_dir+'test/{}.jpg'.format(id_))\n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        x_batch.append(img)\n",
    "        \n",
    "    x_batch = np.array(x_batch, np.float32) / 255\n",
    "    preds = model.predict_on_batch(x_batch)\n",
    "    preds = pd.DataFrame({'id': test_X_batch.values, \n",
    "                          'landmarks': preds.tolist()})\n",
    "    submission = pd.concat([submission, preds], axis=0)\n",
    "\n",
    "submission.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 197, 197, 3)       12        \n",
      "_________________________________________________________________\n",
      "vgg19 (Model)                (None, 6, 6, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "hadamard_classifier_1 (Hadam (None, 14951)             14952     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14951)             0         \n",
      "=================================================================\n",
      "Total params: 20,039,348\n",
      "Trainable params: 20,039,342\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hadamard_vgg19'\n",
    "\n",
    "K.clear_session()\n",
    "model = vgg19_model(input_shape=(input_size, input_size, 3), num_classes=num_class, \n",
    "                       weight_path=os.path.join(data_dir, 'weights/{0}.hdf5'.format(model_name)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1805/1805 [08:49<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for start in tqdm(range(0, len(test_X), batch_size)):\n",
    "    x_batch = []\n",
    "    end = min(start + batch_size, len(test_X))\n",
    "    test_X_batch = test_X[start:end]\n",
    "    for id_ in test_X_batch.values:\n",
    "        img = cv2.imread(data_dir+'test/{}.jpg'.format(id_))\n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        x_batch.append(img)\n",
    "        \n",
    "    x_batch = np.array(x_batch, np.float32) / 255\n",
    "    preds = model.predict_on_batch(x_batch)\n",
    "    \n",
    "    idx = submission['id'].isin(test_X_batch)\n",
    "    submission['landmarks'].loc[idx] = submission['landmarks'].loc[idx].apply(lambda x: np.array(x)*0.8)+\\\n",
    "                                                        pd.Series(list(preds))*0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if balance:\n",
    "    def proba2idx(proba, threshold=.5):\n",
    "        max_idx = np.argmax(proba)\n",
    "        if landmark_id_map[max_idx] == 99999:\n",
    "            return ''\n",
    "        else:\n",
    "            return '{0} {1:.5f}'.format(landmark_id_map[max_idx], proba[max_idx])\n",
    "else:\n",
    "    def proba2idx(proba, threshold=0.1):\n",
    "        max_idx = np.argmax(proba)\n",
    "        \n",
    "        if proba[max_idx] < threshold:\n",
    "            return ''\n",
    "        else:\n",
    "            return '{0} {1:.5f}'.format(max_idx, proba[max_idx])\n",
    "\n",
    "        \n",
    "submission['landmarks'] = submission['landmarks'].apply(proba2idx)\n",
    "\n",
    "missing_id = test_df.loc[~test_df['id'].isin(submission['id'])]\n",
    "missing_id.columns = ['id', 'landmarks']\n",
    "missing_id['landmarks'] = ''\n",
    "\n",
    "submission = pd.concat([submission, missing_id], axis=0)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "date = re.sub('-', '', str(datetime.date.today())[5:])\n",
    "submission.to_csv(os.path.join(data_dir, 'submit/sub_{0}_{1}.csv'.format('ensemble', date)), \n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
