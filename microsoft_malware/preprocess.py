import os
import gc
import sys
import argparse
import numpy as np
import pandas as pd
from itertools import combinations
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

from utils import reduce_mem_usage, feature_types
gc.enable()


def load_file(args):
    dtypes = feature_types()
    num_cols = ['Census_InternalPrimaryDiagonalDisplaySizeInInches', 
                'Census_PrimaryDiskTotalCapacity', 
                'Census_SystemVolumeTotalCapacity', 
                'Census_TotalPhysicalRAM', 
                'Census_InternalPrimaryDisplayResolutionHorizontal', 
                'Census_InternalPrimaryDisplayResolutionVertical', 
                'Census_InternalBatteryNumberOfCharges']

    train = pd.read_csv(os.path.join(args.data_dir, 'train.csv.zip'), dtype=dtypes, 
                        compression='zip', low_memory=True)
    test = pd.read_csv(os.path.join(args.data_dir, 'test.csv.zip'), dtype=dtypes, 
                       compression='zip', low_memory=True)

    return train, test, num_cols


def add_date(train, test, args):
    date_dict = np.load(os.path.join(args.data_dir, 'AvSigVersionTimestamps.npy'))[()]
    train['AvSigDate'] = train['AvSigVersion'].map(date_dict)
    test['AvSigDate'] = test['AvSigVersion'].map(date_dict)
    date_dict = np.load(os.path.join(args.data_dir, 'OSVersionTimestamps.npy'))[()]
    train['OsDate'] = train['Census_OSVersion'].map(date_dict)
    test['OsDate'] = test['Census_OSVersion'].map(date_dict)
    train['Date'] = train[['AvSigDate', 'OsDate']].apply(np.nanmax, axis=1)
    test['Date'] = test[['AvSigDate', 'OsDate']].apply(np.nanmax, axis=1)
    train.drop(['AvSigDate', 'OsDate'], axis=1, inplace=True)

    return train, test


def get_private_idx(test, sep_date='2018-10-26'):
    private_idx = test.loc[(test['AvSigDate'] >= sep_date) | 
                           (test['OsDate'] >= sep_date)].index

    test.drop(['AvSigDate', 'OsDate'], axis=1, inplace=True)

    return private_idx


def get_w2v_feats(train, test):
    def cosine_similarity(x1, x2):
        return np.dot(x1, x2)/(np.linalg.norm(x1)*np.linalg.norm(x2))

    # Create documents
    cols_to_use = ['EngineVersion', 'AppVersion', 'Date']
    doc = pd.concat([train[cols_to_use], test[cols_to_use]], axis=0)
    doc.sort_values('Date', inplace=True)

    sentences = []
    dates = pd.date_range(start='2015-05-01', end='2018-12-01', freq='M')

    for i in range(len(dates)-1):
        d = doc[cols_to_use[:-1]].loc[(doc['Date'] >= dates[i].to_pydatetime()) & 
                                      (doc['Date'] < dates[i+1].to_pydatetime())]\
                                 .drop_duplicates().as_matrix().flatten().tolist()
        if d: sentences.append(d)
    del doc, dates, d; gc.collect()

    # Train word2vec
    model = Word2Vec(sentences, size=5, window=10, min_count=1, sample=.1, workers=5, iter=10)

    # Get train and test features
    w2v_df = train['AppVersion'].astype('object').apply(lambda x: model.wv[x])
    w2v_df = w2v_df.rename('w2vApp').to_frame()
    w2v_df['w2vEngine'] = train['EngineVersion'].astype('object').apply(lambda x: model.wv[x])
    w2v_df['CosSim'] = w2v_df.apply(lambda x: cosine_similarity(x['w2vApp'], x['w2vEngine']), axis=1)
    w2v_df = pd.concat([w2v_df, pd.DataFrame(w2v_df['w2vApp'].values.tolist()).add_prefix('w2vApp_')], axis=1)
    w2v_df = pd.concat([w2v_df, pd.DataFrame(w2v_df['w2vEngine'].values.tolist()).add_prefix('w2vEng_')], axis=1)
    w2v_df.drop(['w2vApp', 'w2vEngine'], axis=1, inplace=True)
    train = pd.concat([train, w2v_df], axis=1)

    w2v_df = test['AppVersion'].astype('object').apply(lambda x: model.wv[x])
    w2v_df = w2v_df.rename('w2vApp').to_frame()
    w2v_df['w2vEngine'] = test['EngineVersion'].astype('object').apply(lambda x: model.wv[x])
    w2v_df['CosSim'] = w2v_df.apply(lambda x: cosine_similarity(x['w2vApp'], x['w2vEngine']), axis=1)
    w2v_df = pd.concat([w2v_df, pd.DataFrame(w2v_df['w2vApp'].values.tolist()).add_prefix('w2vApp_')], axis=1)
    w2v_df = pd.concat([w2v_df, pd.DataFrame(w2v_df['w2vEngine'].values.tolist()).add_prefix('w2vEng_')], axis=1)
    w2v_df.drop(['w2vApp', 'w2vEngine'], axis=1, inplace=True)
    test = pd.concat([test, w2v_df], axis=1)
    w2v_df_cols = w2v_df.columns.tolist()

    return train, test, w2v_df_cols


def get_date_feats(train, test):
    num_cols = []
    train['AvSigVersion2'] = train['AvSigVersion'].apply(lambda x: '.'.join(x.split('.')[:2]))
    train['Census_OSVersion3'] = train['Census_OSVersion'].apply(lambda x: '.'.join(x.split('.')[:3]))
    test['AvSigVersion2'] = test['AvSigVersion'].apply(lambda x: '.'.join(x.split('.')[:2]))
    test['Census_OSVersion3'] = test['Census_OSVersion'].apply(lambda x: '.'.join(x.split('.')[:3]))

    grouped_cols = ['ProductName', 'EngineVersion', 'AppVersion', 'OsBuildLab', 'AvSigVersion2', 
                    'Census_OSVersion3']

    for col in grouped_cols:
        usecol = ''.join([col, 'Position'])
        dropcats = train[col].value_counts()
        dropcats = dropcats[dropcats < 1000].index
        
        tmp = train[col].mask(train[col].isnull(), 'Null')
        train[usecol] = train.groupby(tmp)['Date'].transform(lambda x: x-x.min())    
        train[usecol] = train[usecol].apply(lambda x: x.days)
        train[usecol].loc[train[usecol].isin(dropcats)] = np.nan
        tmp = test[col].mask(test[col].isnull(), 'Null')
        test[usecol] = test.groupby(tmp)['Date'].transform(lambda x: x-x.min())
        test[usecol] = test[usecol].apply(lambda x: x.days)
        test[usecol].loc[test[usecol].isin(dropcats)] = np.nan
        num_cols.append(usecol)
        del dropcats, usecol

    train.drop(['Date', 'AvSigVersion2', 'Census_OSVersion3'], axis=1, inplace=True)
    test.drop(['Date', 'AvSigVersion2', 'Census_OSVersion3'], axis=1, inplace=True)
    
    return train, test, num_cols 


def get_intersect_feats(train, test):
    comb_cols = ['SmartScreen', 'Platform', 'Processor', 'Wdft_IsGamer', 'AVProductsInstalled']
    for (col1, col2) in combinations(comb_cols, r=2):
        usecol = '*'.join([col1, col2])
        tmp_col1, tmp_col2 = '_'.join([col1, 'tmp']), '_'.join([col2, 'tmp'])
        train[tmp_col1], train[tmp_col2] = train[col1].astype('str'), train[col2].astype('str')
        test[tmp_col1], test[tmp_col2] = test[col1].astype('str'), test[col2].astype('str')
        
        train[usecol] = train[tmp_col1] + '*' + train[tmp_col2]
        test[usecol] = test[tmp_col1] + '*' + test[tmp_col2]
        del train[tmp_col1], train[tmp_col2], test[tmp_col1], test[tmp_col2]

    return train, test


def drop_unsimilarity(train, test, num_cols, private_idx):
    # Credit to https://www.kaggle.com/bogorodvo/lightgbm-baseline-model-using-sparse-matrix
    le_cols = list(set(train.columns) - set(['MachineIdentifier', 'HasDetections']))
    for usecol in le_cols:
        if usecol not in num_cols:
            train[usecol] = train[usecol].astype('str')
            test[usecol] = test[usecol].astype('str')

            #Fit LabelEncoder
            le = LabelEncoder().fit(
                    np.unique(train[usecol].unique().tolist()+
                              test[usecol].unique().tolist()))

            #At the end 0 will be used for dropped values
            train[usecol] = le.transform(train[usecol])+1
            test[usecol] = le.transform(test[usecol])+1
            del le
        else:
            # Divide numerical value into bins
            n_bins = train[usecol].nunique() // 5
            comb = pd.concat([train[usecol], test[usecol]], axis=0)
            comb = pd.qcut(comb.values, n_bins, duplicates='drop').codes+1
            train[usecol] = comb[:len(train)]
            test[usecol] = comb[len(train):]
            del comb
        gc.collect()
            
        agg_tr = (train\
                  .groupby([usecol])\
                  .aggregate({'MachineIdentifier':'count'})\
                  .reset_index()\
                  .rename({'MachineIdentifier':'Train'}, axis=1))
        agg_te = (test\
                  .groupby([usecol])\
                  .aggregate({'MachineIdentifier':'count'})\
                  .reset_index()\
                  .rename({'MachineIdentifier':'Test'}, axis=1))
        agg_pr = (test\
                  .iloc[private_idx]\
                  .groupby([usecol])\
                  .aggregate({'MachineIdentifier':'count'})\
                  .reset_index()\
                  .rename({'MachineIdentifier':'Private'}, axis=1))
        
        agg = pd.merge(agg_tr, agg_te, on=usecol, how='outer').replace(np.nan, 0)
        agg = pd.merge(agg, agg_pr, on=usecol, how='outer').replace(np.nan, 0)
        #Select values with more than 1000 observations
        agg = agg[(agg['Train'] > 1000)].reset_index(drop=True)
        agg['Total'] = agg['Train'] + agg['Test']
        agg['ExPublic'] = agg['Train'] + agg['Private']
        #Drop unbalanced values
        agg = agg[(agg['Train'] / agg['Total'] > 0.2) & (agg['Train'] / agg['Total'] < 0.8)]
        agg = agg[(agg['Train'] / agg['ExPublic'] > 0.45) & (agg['Train'] / agg['ExPublic'] < 0.93)]
        agg[usecol+'Copy'] = agg[usecol]    
        
        train[usecol] = (pd.merge(train[[usecol]], 
                                  agg[[usecol, usecol+'Copy']], 
                                  on=usecol, how='left')[usecol+'Copy']\
                         .replace(np.nan, 0).astype('int').astype('category'))

        test[usecol]  = (pd.merge(test[[usecol]], 
                                  agg[[usecol, usecol+'Copy']], 
                                  on=usecol, how='left')[usecol+'Copy']\
                         .replace(np.nan, 0).astype('int').astype('category'))
        del agg_tr, agg_te, agg, usecol

    return train, test


def to_gzip(train, test, args):
    train.to_csv(os.path.join(args.data_dir, args.name_prefix+'_train.csv.gz'), 
                 compression='gzip', index=False)
    test.to_csv(os.path.join(args.data_dir, args.name_prefix+'_test.csv.gz'), 
                compression='gzip', index=False)


def main(args):
    # Read data
    train, test, num_cols = load_file(args)
    # Append datetime feature
    train, test = add_date(train, test, args)
    # Get private test index (date later then or equal with 2018-10-26)
    private_idx = get_private_idx(test)
    # Train Word2Vec and extract features
    train, test, w2v_cols = get_w2v_feats(train, test)
    num_cols += w2v_cols
    # Feature engineering - Count how many days after release date  
    train, test, num_feats = get_date_feats(train, test)
    num_cols += num_feats
    # Get intersect features
    train, test = get_intersect_feats(train, test)
    # Dealing with significant difference between training distribution and test
    train, test = drop_unsimilarity(train, test, num_cols, private_idx)
    # Write DataFrame to compressed csv file 
    to_gzip(train, test, args)


def parse_arguments(argv):
    parser = argparse.ArgumentParser()

    parser.add_argument('--data_dir', type=str,
                        default='/disk/ms_malware/',
                        help='The directory of all csv files and to save the result.')
    parser.add_argument('--name_prefix', type=str,
                        default='preprocessed',
                        help='Prefix for output file name.')

    return parser.parse_args(argv)


if __name__ == '__main__':
    main(parse_arguments(sys.argv[1:]))
