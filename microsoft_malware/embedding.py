import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adam


class Manifold(BaseEstimator, TransformerMixin):
    def __init__(self, n_components, batch_size=512, epochs=20, n_layers=4, frac=2):
        self.n_components = n_components
        self.batch_size = batch_size
        self.epochs = epochs
        self.n_layers = n_layers
        self.frac = frac
    
    def fit(self, X, verbose=2):
        self.num_col = X.shape[1]
        self.model = self.autoencoder(n_layers=self.n_layers, frac=self.frac)
        self.scaler = MinMaxScaler()
        
        X = self.scaler.fit_transform(X)
        self.model.fit(X, X, batch_size=self.batch_size, epochs=self.epochs, verbose=verbose)
        return self
    
    def transform(self, X):
        X = self.scaler.transform(X)
        self.model.predict(X)
        get_layer = K.function([self.model.layers[0].input, K.learning_phase()], 
                               [self.model.layers[2*(self.n_layers-1)].output])
        output = get_layer([X, 1])[0]
        return np.asarray(output)
    
    def autoencoder(self, n_layers=3, frac=2):
        K.clear_session()
        model = Sequential()
        model.add(Dense(self.num_col//frac, input_dim=self.num_col, kernel_initializer='he_normal'))
        model.add(BatchNormalization())
        # Encode
        for i in range(2, n_layers):
            model.add(Dense(self.num_col//frac**i, kernel_initializer='he_normal', activation='elu'))
            model.add(BatchNormalization()) 
            
        model.add(Dense(self.n_components, kernel_initializer='he_normal', activation='elu'))
        model.add(BatchNormalization())
        
        # Decode
        for i in range(n_layers-1, 0, -1):
            model.add(Dense(self.num_col//frac**i, kernel_initializer='he_normal', activation='elu'))
            model.add(BatchNormalization())

        model.add(Dense(self.num_col, kernel_initializer='he_normal', activation='elu'))

        adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
        model.compile(loss='mean_squared_error', optimizer=adam)
        
        return model
    

class BinaryEncoder(LabelEncoder, TransformerMixin):
    def __fill_zero(self, x, max_x):
        return [int(i) for i in bin(x)[2:].zfill(max_x)]
    
    def fit(self, y):
        """Fit LabelEncoder
        Arg:
            y: array-like of shape (n_samples,).
        Return:
            self : returns an instance of self.
        """
        return super().fit(y)
    
    def transform(self, y):
        """Transform labels to binary matrixs
        Arg:
            y: array-like of shape (n_samples,).
        Return:
            y: array-like of shape (n_samples, bin_len)
        """
        label_enc = super().transform(y) + 1
        bin_len = int(np.log2(np.max(label_enc))) + 1

        return list(map(self.__fill_zero, label_enc, [bin_len]*len(y)))
        
    def fit_transform(self, y):
        self.fit(y)
        
        return self.transform(y)
